import subprocess
import urllib.parse
import os
import time
import json
import random  # æ–°å¢ï¼šç”¨äºç”Ÿæˆéšæœºå»¶è¿Ÿå’ŒUA
from datetime import datetime

# æ–°å¢ï¼šå¸¸è§æµè§ˆå™¨UAå¤´æ± ï¼ˆå¯è‡ªè¡Œæ‰©å±•ï¼Œè¶Šå¤šè¶Šéš¾è¢«è¯†åˆ«ï¼‰
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/128.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.6 Safari/605.1.15",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36"
]


def get_random_ua():
    """éšæœºè·å–ä¸€ä¸ªUAå¤´"""
    return random.choice(USER_AGENTS)


def get_target_file_input():
    """è·å–ç›®æ ‡åˆ—è¡¨æ–‡ä»¶è·¯å¾„ï¼ˆé€»è¾‘ä¸å˜ï¼‰"""
    while True:
        file_path = input("è¯·è¾“å…¥ç›®æ ‡åˆ—è¡¨æ–‡ä»¶è·¯å¾„ï¼ˆæ¯è¡Œä¸€ä¸ªç›®æ ‡ï¼‰ï¼š").strip()
        if not file_path:
            print("æ–‡ä»¶è·¯å¾„ä¸èƒ½ä¸ºç©ºï¼è¯·é‡æ–°è¾“å…¥ï¼š")
            continue
        if not os.path.exists(file_path):
            print(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼š{file_path}ï¼è¯·é‡æ–°è¾“å…¥ï¼š")
            continue
        if not os.path.isfile(file_path):
            print(f"{file_path} ä¸æ˜¯æœ‰æ•ˆæ–‡ä»¶ï¼è¯·é‡æ–°è¾“å…¥ï¼š")
            continue
        with open(file_path, "r", encoding="utf-8") as f:
            targets = [line.strip() for line in f if line.strip()]
        if not targets:
            print(f"æ–‡ä»¶ {file_path} ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆç›®æ ‡ï¼è¯·æ£€æŸ¥æ–‡ä»¶å†…å®¹ï¼š")
            continue
        print(f"\næˆåŠŸè¯»å–ç›®æ ‡åˆ—è¡¨ï¼ˆå…± {len(targets)} ä¸ªæœ‰æ•ˆç›®æ ‡ï¼‰ï¼š")
        for i, target in enumerate(targets, 1):
            print(f"  {i}. {target}")
        return file_path, targets


def build_curl_command(resource_type, search_keyword, base_url="http://127.0.0.1:16181/query"):
    """ä¿®æ”¹ï¼šcurlå‘½ä»¤æ·»åŠ éšæœºUAå¤´"""
    encoded_keyword = urllib.parse.quote(search_keyword, safe='')
    url = f"{base_url}/{resource_type}?search={encoded_keyword}&pageSize=1000"
    random_ua = get_random_ua()  # éšæœºé€‰ä¸€ä¸ªUA
    # æ–°å¢ -A å‚æ•°æŒ‡å®šUAå¤´ï¼Œ--connect-timeout å¢åŠ è¿æ¥è¶…æ—¶ï¼ˆé¿å…å¿«é€Ÿå¤±è´¥è¢«è¯†åˆ«ï¼‰
    return f'curl -A "{random_ua}" --connect-timeout 10 --max-time 60 "{url}"'


def log_to_file(message, resource_type, target=""):
    """æ—¥å¿—è®°å½•ï¼ˆé€»è¾‘ä¸å˜ï¼‰"""
    log_dir = "request_logs"
    os.makedirs(log_dir, exist_ok=True)
    log_file = f"{log_dir}/{resource_type}_{datetime.now().strftime('%Y%m%d')}.log"
    target_prefix = f"[ç›®æ ‡ï¼š{target}] " if target else ""
    with open(log_file, "a", encoding="utf-8") as f:
        f.write(f"[{datetime.now().strftime('%H:%M:%S')}] {target_prefix}{message}\n")


def extract_field(response_content, resource_type, target):
    """æ•°æ®æå–ï¼ˆé€»è¾‘ä¸å˜ï¼‰"""
    if not response_content.strip():
        log_to_file(f"å“åº”å†…å®¹ä¸ºç©º", resource_type, target)
        return [f"ã€{target}ã€‘{resource_type} æœåŠ¡å™¨æœªè¿”å›æœ‰æ•ˆæ•°æ®"]

    try:
        log_to_file(f"åŸå§‹å“åº”ï¼ˆå‰500å­—ç¬¦ï¼‰ï¼š{response_content[:500]}...", resource_type, target)
        json_data = json.loads(response_content)

        if json_data.get("code") != 200:
            log_to_file(f"å“åº”codeé200ï¼ˆå®é™…ï¼š{json_data.get('code')}ï¼‰", resource_type, target)
            return [f"ã€{target}ã€‘{resource_type} æœåŠ¡å™¨å“åº”å¤±è´¥ï¼ˆcodeï¼š{json_data.get('code')}ï¼‰"]
        if "params" not in json_data:
            log_to_file("å“åº”ç¼ºå°‘paramså­—æ®µ", resource_type, target)
            return [f"ã€{target}ã€‘{resource_type} å“åº”æ ¼å¼é”™è¯¯ï¼ˆç¼ºå°‘paramsï¼‰"]

        list_data = json_data["params"].get("list", [])
        if not isinstance(list_data, list):
            log_to_file(f"params.listä¸æ˜¯æ•°ç»„ï¼ˆå®é™…ç±»å‹ï¼š{type(list_data)}ï¼‰", resource_type, target)
            return [f"ã€{target}ã€‘{resource_type} æ•°æ®æ ¼å¼é”™è¯¯ï¼ˆlistä¸æ˜¯æ•°ç»„ï¼‰"]
        log_to_file(f"æœ‰æ•ˆlisté•¿åº¦ï¼š{len(list_data)}", resource_type, target)

        field_map = {"web": "domain", "app": "serviceName", "mapp": "serviceName"}
        target_field = field_map[resource_type]
        fields = []
        valid_count = 0
        invalid_count = 0

        for idx, item in enumerate(list_data):
            value = item.get(target_field, "").strip()
            if value:
                fields.append(value)
                valid_count += 1
            else:
                invalid_count += 1
        log_to_file(f"æå–ç»Ÿè®¡ï¼šæœ‰æ•ˆ{valid_count}ä¸ªï¼Œæ— æ•ˆ{invalid_count}ä¸ª", resource_type, target)

        if not fields:
            log_to_file(f"æœªæå–åˆ°ä»»ä½•{target_field}", resource_type, target)
            return [f"ã€{target}ã€‘{resource_type} æœªæ‰¾åˆ°æœ‰æ•ˆ{target_field}"]
        return fields

    except json.JSONDecodeError as e:
        log_to_file(f"JSONè§£æå¤±è´¥ï¼š{str(e)}ï¼ŒåŸå§‹å“åº”ï¼š{response_content[:200]}", resource_type, target)
        return [f"ã€{target}ã€‘{resource_type} æ•°æ®è§£æå¤±è´¥ï¼ˆJSONæ ¼å¼é”™è¯¯ï¼‰"]
    except Exception as e:
        log_to_file(f"æå–å¼‚å¸¸ï¼š{str(e)}", resource_type, target)
        return [f"ã€{target}ã€‘{resource_type} æå–å¤±è´¥ï¼š{str(e)}"]


def run_curl_and_save(resource_type, target, output_file):
    """æ‰§è¡Œcurlè¯·æ±‚ï¼ˆé€»è¾‘ä¸å˜ï¼ŒUAå·²åœ¨build_curl_commandä¸­æ·»åŠ ï¼‰"""
    max_retries = 2
    retry_count = 0

    while retry_count <= max_retries:
        try:
            curl_cmd = build_curl_command(resource_type, target)
            log_to_file(f"æ‰§è¡Œå‘½ä»¤ï¼š{curl_cmd}", resource_type, target)
            print(f"\n=== å¤„ç†ç›®æ ‡ã€{target}ã€‘- èµ„æºç±»å‹ã€{resource_type}ã€‘ï¼ˆé‡è¯•ï¼š{retry_count}ï¼‰===")
            print(f"æ‰§è¡Œå‘½ä»¤ï¼š{curl_cmd}")

            result = subprocess.run(
                curl_cmd,
                shell=True,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                timeout=60
            )

            if result.returncode != 0:
                log_to_file(f"curlæ‰§è¡Œå¤±è´¥ï¼ˆçŠ¶æ€ç ï¼š{result.returncode}ï¼Œé”™è¯¯ï¼š{result.stderr}ï¼‰", resource_type, target)
                retry_count += 1
                if retry_count <= max_retries:
                    # æ–°å¢ï¼šé‡è¯•å‰éšæœºå»¶è¿Ÿï¼ˆ5-10ç§’ï¼‰ï¼Œé¿å…å›ºå®šé‡è¯•é—´éš”è¢«è¯†åˆ«
                    retry_delay = random.randint(5, 10)
                    print(f"âš ï¸ è¯·æ±‚å¤±è´¥ï¼Œ{max_retries - retry_count + 1}æ¬¡é‡è¯•ä¸­ï¼ˆç­‰å¾…{retry_delay}ç§’ï¼‰...")
                    time.sleep(retry_delay)
                continue

            response_content = result.stdout
            extracted_values = extract_field(response_content, resource_type, target)
            target_field = "domain" if resource_type == "web" else "serviceName"

            append_content = f"\n{'=' * 50}\n"
            append_content += f"ã€ç›®æ ‡ï¼š{target}ã€‘{resource_type} ç±»å‹ {target_field} åˆ—è¡¨ï¼ˆ{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}ï¼‰\n"
            append_content += f"{'=' * 50}\n"
            append_content += "\n".join(extracted_values)
            append_content += "\n"

            with open(output_file, "a+", encoding="utf-8") as f:
                f.write(append_content)

            log_to_file(f"æå–å®Œæˆï¼Œå…±{len(extracted_values)}ä¸ª{target_field}ï¼ˆå·²è¿½åŠ åˆ°æ–‡ä»¶ï¼‰", resource_type, target)
            print(f"âœ… ã€{target}ã€‘-ã€{resource_type}ã€‘ç»“æœå·²è¿½åŠ åˆ°ï¼š{os.path.abspath(output_file)}")
            print(f"   å…±æå–åˆ° {len(extracted_values)} ä¸ª{target_field}")
            return result.returncode

        except subprocess.TimeoutExpired:
            log_to_file(f"curlå‘½ä»¤è¶…æ—¶ï¼ˆ60ç§’ï¼‰", resource_type, target)
            retry_count += 1
            if retry_count <= max_retries:
                retry_delay = random.randint(5, 10)
                print(f"âš ï¸ è¯·æ±‚è¶…æ—¶ï¼Œ{max_retries - retry_count + 1}æ¬¡é‡è¯•ä¸­ï¼ˆç­‰å¾…{retry_delay}ç§’ï¼‰...")
                time.sleep(retry_delay)
            continue
        except Exception as e:
            log_to_file(f"è¯·æ±‚å¼‚å¸¸ï¼š{str(e)}", resource_type, target)
            retry_count += 1
            if retry_count <= max_retries:
                retry_delay = random.randint(5, 10)
                print(f"âš ï¸ è¯·æ±‚å¼‚å¸¸ï¼Œ{max_retries - retry_count + 1}æ¬¡é‡è¯•ä¸­ï¼ˆç­‰å¾…{retry_delay}ç§’ï¼‰...")
                time.sleep(retry_delay)
            continue

    log_to_file(f"è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆ{max_retries}æ¬¡ï¼‰ï¼Œè¯·æ±‚å¤±è´¥", resource_type, target)
    print(f"âŒ ã€{target}ã€‘-ã€{resource_type}ã€‘è¯·æ±‚å¤±è´¥ï¼ˆå·²é‡è¯•{max_retries}æ¬¡ï¼‰")
    return -1


def main():
    resource_config = [("web", "web_results.txt"), ("app", "app_results.txt"), ("mapp", "mapp_results.txt")]
    # ä¿®æ”¹ï¼šå»¶è¿Ÿæ”¹ä¸ºâ€œåŸºç¡€å€¼+éšæœºå€¼â€ï¼ˆé¿å…å›ºå®šé—´éš”è¢«è¯†åˆ«ï¼‰
    base_target_delay = 60  # ç›®æ ‡é—´åŸºç¡€å»¶è¿Ÿï¼ˆç§’ï¼‰
    target_delay_range = 50  # éšæœºæ³¢åŠ¨èŒƒå›´ï¼ˆç§’ï¼‰ï¼šæ€»å»¶è¿Ÿ = 100~150ç§’
    base_resource_delay = 20  # èµ„æºé—´åŸºç¡€å»¶è¿Ÿï¼ˆç§’ï¼‰
    resource_delay_range = 10  # éšæœºæ³¢åŠ¨èŒƒå›´ï¼ˆç§’ï¼‰ï¼šæ€»å»¶è¿Ÿ = 20~30ç§’

    try:
        file_path, targets = get_target_file_input()
        total_targets = len(targets)
        print(f"\nå³å°†å¼€å§‹æ‰¹é‡å¤„ç†ï¼ˆç›®æ ‡æ•°ï¼š{total_targets}ï¼Œèµ„æºç±»å‹æ•°ï¼š{len(resource_config)}ï¼‰")
        print(f"ç›®æ ‡é—´å»¶è¿Ÿï¼š{base_target_delay}~{base_target_delay + target_delay_range}ç§’ï¼ˆéšæœºï¼‰")
        print(f"èµ„æºé—´å»¶è¿Ÿï¼š{base_resource_delay}~{base_resource_delay + resource_delay_range}ç§’ï¼ˆéšæœºï¼‰\n")

        for target_idx, target in enumerate(targets, 1):
            print(f"ğŸ“Œ å¼€å§‹å¤„ç†ç¬¬ {target_idx}/{total_targets} ä¸ªç›®æ ‡ï¼š{target}")

            for res_idx, (resource_type, output_file) in enumerate(resource_config):
                run_curl_and_save(resource_type, target, output_file)

                # èµ„æºé—´éšæœºå»¶è¿Ÿï¼ˆæœ€åä¸€ä¸ªç±»å‹ä¸é—´éš”ï¼‰
                if res_idx < len(resource_config) - 1:
                    resource_delay = random.randint(base_resource_delay, base_resource_delay + resource_delay_range)
                    print(f"\nâŒ› ç­‰å¾…{resource_delay}ç§’åå¤„ç†ä¸‹ä¸€ä¸ªèµ„æºç±»å‹...")
                    time.sleep(resource_delay)

            # ç›®æ ‡é—´éšæœºå»¶è¿Ÿï¼ˆæœ€åä¸€ä¸ªç›®æ ‡ä¸é—´éš”ï¼‰
            if target_idx < total_targets:
                target_delay = random.randint(base_target_delay, base_target_delay + target_delay_range)
                print(f"\nâŒ› ç­‰å¾…{target_delay}ç§’åå¤„ç†ä¸‹ä¸€ä¸ªç›®æ ‡...\n" + "-" * 80)
                time.sleep(target_delay)

        # å¤„ç†å®Œæˆï¼Œè‡ªåŠ¨è°ƒç”¨Shellè„šæœ¬ï¼ˆé€»è¾‘ä¸å˜ï¼‰
        print(f"\nğŸ‰ æ‰€æœ‰ç›®æ ‡å¤„ç†å®Œæˆï¼")
        print(f"ğŸ“ ç»“æœæ–‡ä»¶åˆ—è¡¨ï¼š")
        for _, output_file in resource_config:
            print(f"   - {os.path.abspath(output_file)}")
        print(f"ğŸ“Š è¯¦ç»†æ—¥å¿—ç›®å½•ï¼š{os.path.abspath('request_logs')}")

        # è‡ªåŠ¨è°ƒç”¨Shellè„šæœ¬ï¼ˆä¸ä¹‹å‰ä¸€è‡´ï¼Œç¡®ä¿Shellè„šæœ¬å·²åŒæ­¥ä¿®æ”¹ï¼‰
        print(f"\n==================================================")
        print(f"å¼€å§‹è‡ªåŠ¨æ‰§è¡Œå­åŸŸåæ”¶é›†ï¼ˆè°ƒç”¨Shellè„šæœ¬ï¼‰...")
        print(f"==================================================")
        shell_script_name = "oneforall.sh"  # å¯¹åº”ä¿®æ”¹åçš„Shellè„šæœ¬å
        current_dir = os.path.dirname(os.path.abspath(__file__))
        shell_script_path = os.path.join(current_dir, shell_script_name)

        if not os.path.exists(shell_script_path):
            print(f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°Shellè„šæœ¬ï¼è·¯å¾„ï¼š{shell_script_path}")
            exit(1)

        if not os.access(shell_script_path, os.X_OK):
            print(f"âš ï¸ Shellè„šæœ¬ç¼ºå°‘æ‰§è¡Œæƒé™ï¼Œæ­£åœ¨è‡ªåŠ¨æ·»åŠ ...")
            subprocess.run(["chmod", "+x", shell_script_path], check=True)
            print(f"âœ… å·²ä¸ºShellè„šæœ¬æ·»åŠ æ‰§è¡Œæƒé™")

        print(f"ğŸ“Œ æ­£åœ¨è¿è¡ŒShellè„šæœ¬ï¼š{shell_script_path}")
        shell_result = subprocess.run(
            [shell_script_path],
            shell=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            encoding="utf-8"
        )

        print(f"\n--- Shellè„šæœ¬æ‰§è¡Œè¾“å‡º ---")
        print(shell_result.stdout)
        if shell_result.stderr:
            print(f"--- Shellè„šæœ¬é”™è¯¯è¾“å‡º ---")
            print(shell_result.stderr)

        if shell_result.returncode == 0:
            print(f"\nâœ… å­åŸŸåæ”¶é›†è„šæœ¬æ‰§è¡Œå®Œæˆï¼ç»“æœä¿å­˜åœ¨ï¼š~/xinxi/OneForAll-master/results")
        else:
            print(f"\nâŒ å­åŸŸåæ”¶é›†è„šæœ¬æ‰§è¡Œå¤±è´¥ï¼ˆçŠ¶æ€ç ï¼š{shell_result.returncode}ï¼‰")

    except Exception as e:
        print(f"\nâŒ ç¨‹åºå¼‚å¸¸ç»ˆæ­¢ï¼š{str(e)}")
        exit(1)


if __name__ == "__main__":
    main()
